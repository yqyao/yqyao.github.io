<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yqyao.github.io">
  <title>RankDataset | sound</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="RankDataset：超大规模数据集加载利器问题阐述小王是一名炼丹术士，某一天小王逛着arxiv的时候，突然眼前一亮，发现一篇很好的论文:CLIP，看着论文开源的github，小王撸起袖子，准备自己爬一批数据尝试训一下clip。经过N久之后，终于凑齐了4亿数据。 虽然没经过清洗，不过小王践行实践原则，准备先暴力开搞一下。小王使用了PyTorch框架，写完了build模型，把之前的Dataset拿">
<meta property="og:type" content="article">
<meta property="og:title" content="RankDataset">
<meta property="og:url" content="http://yqyao.github.io/2020/12/30/rankdataset/index.html">
<meta property="og:site_name" content="sound">
<meta property="og:description" content="RankDataset：超大规模数据集加载利器问题阐述小王是一名炼丹术士，某一天小王逛着arxiv的时候，突然眼前一亮，发现一篇很好的论文:CLIP，看着论文开源的github，小王撸起袖子，准备自己爬一批数据尝试训一下clip。经过N久之后，终于凑齐了4亿数据。 虽然没经过清洗，不过小王践行实践原则，准备先暴力开搞一下。小王使用了PyTorch框架，写完了build模型，把之前的Dataset拿">
<meta property="og:locale">
<meta property="article:published_time" content="2020-12-29T16:00:00.000Z">
<meta property="article:modified_time" content="2022-12-03T07:22:29.209Z">
<meta property="article:author" content="姚勇强">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="sound" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/back.jpg">
  
  
<link rel="stylesheet" href="/main.css?v=4.0.0.css">

  

  
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?[object Object]";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/avatar.png" class="js-avatar">
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>

		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/archives">归档</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a data-idx="0" q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
    			
            
    			
    			<a data-idx="1" q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/yqyao" title="github"><i class="icon-github"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-sort"></i></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/img/avatar.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">归档</a></li>
		        
		        
		        	<li><a href="/archives/">所有文章</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/yqyao" title="github"><i class="icon-github"></i></a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-rankdataset" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      RankDataset
    </h1>
  

        <a href="/2020/12/30/rankdataset/" class="archive-article-date">
  	<time datetime="2020-12-29T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2020-12-30</time>
</a>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="RankDataset：超大规模数据集加载利器"><a href="#RankDataset：超大规模数据集加载利器" class="headerlink" title="RankDataset：超大规模数据集加载利器"></a>RankDataset：超大规模数据集加载利器</h1><h2 id="问题阐述"><a href="#问题阐述" class="headerlink" title="问题阐述"></a>问题阐述</h2><p>小王是一名炼丹术士，某一天小王逛着arxiv的时候，突然眼前一亮，发现一篇很好的论文:CLIP，看着论文开源的github，小王撸起袖子，准备自己爬一批数据尝试训一下clip。经过N久之后，终于凑齐了4亿数据。 虽然没经过清洗，不过小王践行实践原则，准备先暴力开搞一下。小王使用了PyTorch框架，写完了build模型，把之前的Dataset拿过来抄了一下，写了个RandomSampler，用了官方的Dataloader，一切就绪之后，一份伪Code就写好了： (如果你不熟悉 Dataset和Sampler的具体含义，可以参考这里Dataset） 下图是一个简化后的加载示意图</p>
<ul>
<li>meta file 格式</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">filename label</span> </span><br><span class="line">image1.jpg &quot;balabala&quot;</span><br><span class="line">image2.jpg &quot;balabala&quot;</span><br><span class="line">image3.jpg &quot;balabala&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li>NaiveDataset</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> Sampler</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NaiveDataset</span>(<span class="title class_ inherited__">Dataset</span>):    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, meta_file</span>):</span><br><span class="line">        <span class="built_in">super</span>(NaiveDataset, self).__init__()</span><br><span class="line">        self.metas = self.parse(meta_file)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, meta_file</span>):</span><br><span class="line">        metas = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(meta_file) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">                metas.append(line.strip())</span><br><span class="line">        <span class="keyword">return</span> metas</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.metas[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.metas)</span><br></pre></td></tr></table></figure>


<ul>
<li>RandomSampler</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RandomSampler</span>(<span class="title class_ inherited__">Sampler</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Samples elements randomly, without replacement.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset</span>):</span><br><span class="line">        self.dataset = dataset</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randperm(<span class="built_in">len</span>(self.dataset)).tolist())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dataset)</span><br></pre></td></tr></table></figure>

<ul>
<li>训练流程</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">dataset = NaiveDataset(<span class="string">&quot;/path/to/meta&quot;</span>)</span><br><span class="line">sampler = RandomSampler(datset)</span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">            dataset=dataset,</span><br><span class="line">            batch_size=<span class="number">32</span>,</span><br><span class="line">            shuffle=<span class="literal">False</span>,</span><br><span class="line">            num_workers=<span class="number">4</span>,</span><br><span class="line">            sampler=sampler</span><br><span class="line">        )</span><br><span class="line">model = build_model()</span><br><span class="line"><span class="keyword">for</span> index, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">    image, label = batch</span><br><span class="line">    output = model(image)</span><br><span class="line">    loss = criterion(output, label)</span><br><span class="line">    loss.backward()</span><br></pre></td></tr></table></figure>


<p>写完代码之后，小王美滋滋的准备开始训练了一下，先拿一个小训练集测试一下有没有bug，一番修改之后，看着逐渐收敛的网络，小王很开心，准备上大数据集了。 既然要训大数据量，那必然要上分布式训练，好在PyTorch的分布式训练比较容易，小王从表哥家借来了一个8GPU的挖矿机。准备使用world_size为8的分布式训练。 小王在原来的sampler基础上略加修改，就得到了一个新的sampler (分布式sampler，负责分发训练数据index给不同的卡)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DistributedRandomSampler</span>(<span class="title class_ inherited__">Sampler</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Samples elements randomly, without replacement.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, rank, world_size</span>):</span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.world_size = world_size</span><br><span class="line">        self.rank = rank</span><br><span class="line">        self.num_samples = <span class="built_in">int</span>(math.ceil(<span class="built_in">len</span>(self.dataset) * <span class="number">1.0</span> / self.world_size))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        index_list = torch.randperm(<span class="built_in">len</span>(self.dataset)).tolist()</span><br><span class="line">        index_list = padding(<span class="built_in">len</span>(self.dataset), self.rank, self.world_size) <span class="comment">#padding函数保证index_list长度整除rank</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(index_list[self.rank * self.num_samples: (self.rank + <span class="number">1</span>) * self.num_samples])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<ul>
<li>原因分析</li>
</ul>
<p>通常来说我们为了保证训练高效，在分布式训练时我们都会开启多进程，每块卡单独一个进程。每个进程里面会存储一些基本的模型和优化器信息，当然也会存储我们训练metas信息。<br> 在原生的PyTorch 数据集加载过程中，我们的分布式sampler 负责给每块卡分发index，为了保证高效读取，每个进程都需要保存其所有的metas。 那么对于8卡任务也就是会有8 * metas 需要在内存里存放(实际考虑到dataloader 的worker 数量，这个实际占用量会更大)。<br> 当我们的metas信息比较大的时候，我们的内存就可能会出现溢出问题。<br> 之前没有训练过这个大的数据，这次数据量上来了，内存吃不下很正常。</p>
<h2 id="解决方案一"><a href="#解决方案一" class="headerlink" title="解决方案一"></a>解决方案一</h2><h3 id="server-方案"><a href="#server-方案" class="headerlink" title="server 方案"></a>server 方案</h3><p> 你现在一台机器上要load 8份数据，当然内存要爆了。我在家的时候都是开两台机器，一台专门用来读数据(称为server)，另一台专门用来训练(称为client)。<br> 然后训练的时候client每次取数据都从server获得数据，这样数据只需要在server存一份就够了。</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ServerDataset</span>(<span class="title class_ inherited__">Dataset</span>):    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, meta_file, server_ip, server_port</span>):</span><br><span class="line">        <span class="built_in">super</span>(ServerDataset, self).__init__()</span><br><span class="line">        self.server_ip = server_ip</span><br><span class="line">        self.server_port = server_port</span><br><span class="line">        self.meta_num = get_meta_num(server_ip, server_port)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_meta</span>(<span class="params">self, idx</span>):</span><br><span class="line">        meta = requests.get(<span class="string">&#x27;http://&#123;&#125;:&#123;&#125;/get/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.server_ip, self.server_port, idx), timeout=<span class="number">1000</span>).json()</span><br><span class="line">        <span class="keyword">return</span> meta</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.get_meta(idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.meta_num</span><br></pre></td></tr></table></figure>

<p> <strong>局限性</strong></p>
<p> 看起来蛮简单的，只是把原来的从内存读变成了从server网络读取。可是这样的训练效率怎么样呢？<br> “这种做法对于qps在1k以下还比较实用, 但是当训练的总batchsize 特别大的时候这种做法会有明显的瓶颈问题，受限于中心化的并发读取上限问题，因此此方法具有一定的局限性。”</p>
<h3 id="RankDataset"><a href="#RankDataset" class="headerlink" title="RankDataset"></a>RankDataset</h3><ul>
<li><p>原理分析<br>从原理出发，小王进行了一下计算，其实每张卡实际使用的数据量为 len(metas) &#x2F;&#x2F; world_size, 在一般的训练过程中为了访问方便，采用sampler 去划分不同的卡读取的index，每块卡还是会保留所有的meta信息，因此这样会导致前面的内存问题。 而实际上，我保存了1000的数据，实际只使用其中了125张，那位为什么要把所有的都存下来呢？为什么我不能只把我需要用到的数据读取进来呢？说干就干，小王设计了一下方案</p>
</li>
<li><p>切分流程</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">                     Metas 切分过程, mini_epoch = 2, world_size = 8</span><br><span class="line"></span><br><span class="line">    mini_epoch_idx = 0                            mini_epoch_idx = 1</span><br><span class="line">---- ---- ---- ---- ---- ---- ---- ---- | ---- ---- ---- ---- ---- ---- ---- ---- </span><br><span class="line">rk0  rk1  rk2  rk3  rk4  rk5  rk6  rk7  | rk0  rk1  rk2  rk3  rk4  rk5  rk6  rk7 </span><br><span class="line"></span><br><span class="line">每次只加载 len(metas) // (world_size * mini_epoch) 这样我内存占用就会可以人为的进行调整</span><br></pre></td></tr></table></figure>

<p>基本就是这样了，这样内存就是满足了，可是还有一点，之前的sampler是针对整个数据集来进行的，这里要怎么做呢？略作思索，小王得出来结论：<br> 对于普通的dataloader，随机性一般由sampler进行控制，这里由于已经分rank进行加载meta信息，为了保证不同epoch 加载数据顺序保证随机性，每隔一个epoch需要重新分配一次每个 rank 的 meta 信息。 小王在此基础上写出了新的code。</p>
<ul>
<li>本地读取样例</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RankDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    实际流程</span></span><br><span class="line"><span class="string">    获取rank和world_size 信息 -&gt; 获取dataset长度 -&gt; 根据dataset长度产生随机indices -&gt;</span></span><br><span class="line"><span class="string">    给不同的rank 分配indices -&gt; 根据这些indices产生metas </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, meta_file, world_size, rank, seed</span>):</span><br><span class="line">        <span class="built_in">super</span>(RankDataset, self).__init__()</span><br><span class="line">        random.seed(seed)</span><br><span class="line">        np.random.seed(seed)</span><br><span class="line">        self.world_size = world_size</span><br><span class="line">        self.rank = rank</span><br><span class="line"></span><br><span class="line">        self.metas = self.parse(meta_file)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, meta_file</span>):</span><br><span class="line">        dataset_size = self.get_dataset_size(meta_file)                                     <span class="comment"># 获取metafile的行数</span></span><br><span class="line">        local_rank_index = self.get_local_index(dataset_size, self.rank, self.world_size)   <span class="comment"># 根据world size和rank，获取当前epoch，当前rank需要训练的index。</span></span><br><span class="line">        self.metas = self.read_file(meta_file, local_rank_index)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.metas[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.metas)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>因为这里的dataset读取进来的数据已经是分片之后的了，对应的sampler只需要使用一开始的RandomSampler就可以:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">epoch_num = <span class="number">0</span></span><br><span class="line">dataset = RankDataset(<span class="string">&quot;/path/to/meta&quot;</span>, world_size, rank, seed=epoch_num)</span><br><span class="line">sampler = RandomSampler(datset)</span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">            dataset=dataset,</span><br><span class="line">            batch_size=<span class="number">32</span>,</span><br><span class="line">            shuffle=<span class="literal">False</span>,</span><br><span class="line">            num_workers=<span class="number">4</span>,</span><br><span class="line">            sampler=sampler</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<ul>
<li>特别注意</li>
</ul>
<p>由于每个epoch都要重新读取数据，因此每个epoch要重新build dataloader:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> <span class="built_in">range</span>(epoch_num):</span><br><span class="line"></span><br><span class="line">    dataset = RankDataset(<span class="string">&quot;/path/to/meta&quot;</span>, world_size, rank, seed=epoch_num)</span><br><span class="line">    sampler = RandomSampler(datset)</span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">                dataset=dataset,</span><br><span class="line">                batch_size=<span class="number">32</span>,</span><br><span class="line">                shuffle=<span class="literal">False</span>,</span><br><span class="line">                num_workers=<span class="number">4</span>,</span><br><span class="line">                sampler=sampler</span><br><span class="line">            )</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这样看起来每个epoch都要读取数据很麻烦，但是和4亿数据的训练时间相比，读取的时间便不算什么了。 不过这种方法是否合理呢，会不会影响精度？小王在不同任务上进行了实验，分类任务上用imagenet和imagenet22k数据集，检测任务上使用了Open-Image数据集，均发现没有精度的损失。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="对于一般的数据集"><a href="#对于一般的数据集" class="headerlink" title="对于一般的数据集"></a>对于一般的数据集</h4><ul>
<li>自己实现一个继承torch.data.Dataset类就可以，需要实现init,getitem,len三个函数；  </li>
<li>使用torch默认的RandomSampler即可满足一般的random shuffle需求</li>
<li>使用torch默认的dataloader就制定完成数据迭代器</li>
</ul>
<h4 id="使用分布式训练"><a href="#使用分布式训练" class="headerlink" title="使用分布式训练"></a>使用分布式训练</h4><ul>
<li>Dataset保持不变  </li>
<li>sampler进行修改，保证每个rank读到的index可以覆盖到整个dataset，并且每个rank读的数据要是等量的</li>
<li>dataloader保持不变</li>
</ul>
<h4 id="使用中心化server"><a href="#使用中心化server" class="headerlink" title="使用中心化server"></a>使用中心化server</h4><p>为了解决大数据量加载内存不够的问题，可以专门使用一个节点当做server，为训练集供给训练。好处是可以节省内存，坏处是麻烦，以及对网络带宽和qps有需求。</p>
<ul>
<li>Dataset进行修改，  getitem从内存读取数据改成向server发出请求，获得对应index的数据。</li>
<li>可以直接使用分布式的sampler</li>
<li>dataloader保持不变</li>
</ul>
<h4 id="RankDataset："><a href="#RankDataset：" class="headerlink" title="RankDataset："></a>RankDataset：</h4><p>从原理入手，在分布式的基础上，直接计算每个epoch当前rank需要训练的数据的index。好处是大量的节省内存，且不需要额外开server。坏处是每个epoch都需要重新build dataloader，但是当数据量大的时候这个时间是可以接受的。</p>
<ul>
<li>支持进一步切分数据集，分批去读取数据集。</li>
<li>Dataset进行修改：每个epoch先计算该rank需要使用的index，然后根据index获取meta_file对应行，加载到内存中。</li>
<li>改为torch默认的使用torch默认的RandomSampler即可满足一般的random。</li>
<li>dataloader保持不变，但是在训练过程中，每个epoch到要用不同的随机数重新build dataloader。</li>
</ul>
<p>最后我们来对比一下实际的内存优化效果。</p>
<table>
<thead>
<tr>
<th align="center">方案</th>
<th align="center">PyTorch 官方处理</th>
<th align="center">中心化Meta</th>
<th align="center">RankDataset</th>
</tr>
</thead>
<tbody><tr>
<td align="center">内存占用</td>
<td align="center">M</td>
<td align="center">0</td>
<td align="center">M &#x2F; world_size &#x2F; mini_epoch</td>
</tr>
<tr>
<td align="center">并发</td>
<td align="center">内存读取</td>
<td align="center">网络读取</td>
<td align="center">内存读取</td>
</tr>
</tbody></table>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PyTorch/" rel="tag">PyTorch</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-book icon"></i>
	<a class="article-category-link" href="/categories/%E6%A1%86%E6%9E%B6%E4%BC%98%E5%8C%96/">框架优化</a>
	</div>


      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
    <a href="/2021/08/10/hardware_design_det/" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          Hardware-Design
        
      </div>
    </a>
  
  
    <a href="/2020/12/10/bagoftricks/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Bag-of-Tricks</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>









          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2023 姚勇强
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: true
	}

</script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?cae73264bdf8671f85792a172a2039d0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


<script src="/./main.js?v=4.0.0.js"></script>



    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)">Det</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)">PyTorch</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            2、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: true
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">个人主页&lt;br&gt;&lt;br&gt;记录自己做过的事&lt;br&gt;</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>